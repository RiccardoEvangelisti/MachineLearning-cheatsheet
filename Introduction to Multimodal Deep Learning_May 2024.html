<html lang=en><head>
        <meta charset=utf-8>
        <meta name=viewport content="width=device-width, initial-scale=1.0">
        <meta http-equiv=X-UA-Compatible content="ie=edge">
        <title>Introduction to Multimodal Deep Learning | by Abdulkader Helwan | in Stackademic - Freedium</title>
        
        <meta name=description content="Basics of Multimodal Models">
        <meta name=keywords content="medium, paywall, medium.com, paywall breakthrough">
        <script src="https://cdn.tailwindcss.com"></script>
        <script src="https://cdn.tailwindcss.com?plugins=forms,typography,aspect-ratio"></script>
        <link href="https://glyph.medium.com/css/unbound.css" rel=stylesheet>
        <link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png">
        <link rel=icon type="image/png" sizes=32x32 href="/favicon-32x32.png">
        <link rel=icon type="image/png" sizes=16x16 href="/favicon-16x16.png">
        <link rel=manifest href="/site.webmanifest">
        <link rel=mask-icon href="/safari-pinned-tab.svg" color=#00aba9>
        <meta name=msapplication-TileColor content=#00aba9>
        <meta name=theme-color content=#ffffff>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <link rel=stylesheet href="https://unpkg.com/@highlightjs/cdn-assets@11.8.0/styles/atom-one-dark.min.css">
        <script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.4/dist/lazyload.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/lightense-images@1.0.17/dist/lightense.min.js"></script>
        <script>
            if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
  document.documentElement.classList.add('dark');
  //document.getElementById('darkIcon').classList.remove('hidden');
  //document.getElementById('lightIcon').classList.add('hidden')
} else {
  document.documentElement.classList.remove('dark')
  //document.getElementById('lightIcon').classList.remove('hidden');
  //document.getElementById('darkIcon').classList.add('hidden');
}
        </script>
        <style>
          .overflow-hidden {
              overflow: hidden !important;
          }

        .shadow-lf {
            box-shadow: inset 3px 0 0 0 rgb(209 207 239 / var(--tw-bg-opacity));
        }
        </style>
        <style>
    .notification-container {
      display: none;
      position: fixed;
      top: 20px;
      padding: 2%;
      max-height: 95vh; /* Set a maximum height for the container */
    overflow-y: auto; /* Enable vertical scrolling */
    }

    .notification-card {
      background-color: #fff;
      border: 1px solid #ccc;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      padding: 10px 20px;
      border-radius: 5px;
      text-align: center;
    }
        </style>
        <script>
    window._resizeIframe = function (iframeData)
    {
            iframeData.iframe.height = iframeData.height
            _resizeIframeWidth()
    }

    function _resizeIframeWidth(){ var element = document.querySelector(".main-content");
    var width = element.offsetWidth;

    iframes = document.getElementsByTagName("iframe");
    for (var i = 0; i < iframes.length; i++) {
            iframes[i].width = width
    }

    window.onresize = _resizeIframeWidth
 }
        </script>
        <!--
        <script>
                window.onload = function() {
    window.parent.postMessage({
        type: "URL_UPDATE",
        url: window.location.href
    }, "*");
}
</script>
                -->
    </head>
    <body class="dark:bg-gray-800 bg-white"><div class="fixed bottom-4 left-4" style="z-index: 999999;">
        <button id=openProblemModal class="m-1.5 flex items-center bg-red-500 text-white py-2 px-4 rounded-full shadow-lg hover:bg-red-600 focus:outline-none focus:ring-2 focus:ring-blue-500">
            <svg xmlns="http://www.w3.org/2000/svg" height=1em viewBox="0 0 512 512">
                <!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2023 Fonticons, Inc. --><style>svg{fill:#ffffff}</style>
                <path d="M256 32c14.2 0 27.3 7.5 34.5 19.8l216 368c7.3 12.4 7.3 27.7 .2 40.1S486.3 480 472 480H40c-14.3 0-27.6-7.7-34.7-20.1s-7-27.8 .2-40.1l216-368C228.7 39.5 241.8 32 256 32zm0 128c-13.3 0-24 10.7-24 24V296c0 13.3 10.7 24 24 24s24-10.7 24-24V184c0-13.3-10.7-24-24-24zm32 224a32 32 0 1 0 -64 0 32 32 0 1 0 64 0z"></path>
            </svg>
        </button>
        <button id=darkModeToggle class="m-1.5 flex items-center bg-blue-500 text-white py-2 px-4 rounded-full shadow-lg hover:bg-blue-600 focus:outline-none">
            <svg id=darkIcon xmlns="http://www.w3.org/2000/svg" height=1em viewBox="0 0 384 512">
                <!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2023 Fonticons, Inc. -->
                <path d="M223.5 32C100 32 0 132.3 0 256S100 480 223.5 480c60.6 0 115.5-24.2 155.8-63.4c5-4.9 6.3-12.5 3.1-18.7s-10.1-9.7-17-8.5c-9.8 1.7-19.8 2.6-30.1 2.6c-96.9 0-175.5-78.8-175.5-176c0-65.8 36-123.1 89.3-153.3c6.1-3.5 9.2-10.5 7.7-17.3s-7.3-11.9-14.3-12.5c-6.3-.5-12.6-.8-19-.8z"></path>
            </svg>
            <!-- SVG icon for light mode (e.g., a sun) -->
            <svg class=hidden id=lightIcon xmlns="http://www.w3.org/2000/svg" height=1em viewBox="0 0 512 512">
                <!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2023 Fonticons, Inc. -->
                <path d="M361.5 1.2c5 2.1 8.6 6.6 9.6 11.9L391 121l107.9 19.8c5.3 1 9.8 4.6 11.9 9.6s1.5 10.7-1.6 15.2L446.9 256l62.3 90.3c3.1 4.5 3.7 10.2 1.6 15.2s-6.6 8.6-11.9 9.6L391 391 371.1 498.9c-1 5.3-4.6 9.8-9.6 11.9s-10.7 1.5-15.2-1.6L256 446.9l-90.3 62.3c-4.5 3.1-10.2 3.7-15.2 1.6s-8.6-6.6-9.6-11.9L121 391 13.1 371.1c-5.3-1-9.8-4.6-11.9-9.6s-1.5-10.7 1.6-15.2L65.1 256 2.8 165.7c-3.1-4.5-3.7-10.2-1.6-15.2s6.6-8.6 11.9-9.6L121 121 140.9 13.1c1-5.3 4.6-9.8 9.6-11.9s10.7-1.5 15.2 1.6L256 65.1 346.3 2.8c4.5-3.1 10.2-3.7 15.2-1.6zM160 256a96 96 0 1 1 192 0 96 96 0 1 1 -192 0zm224 0a128 128 0 1 0 -256 0 128 128 0 1 0 256 0z"></path>
            </svg>
        </button>
    </div>
    <div class=notification-container style="z-index: 999999;">
        <div class="notification-card dark:bg-gray-800 bg-white">
            <p class="text-2xl pb-5 text-black dark:text-white">Bad news</p>
            <p class="pb-3 text-black dark:text-white">
                We regret to inform you that our account on BuyMeACoffee has been suspended due to a violation of their terms of service. This was an unexpected development, and we are currently addressing the matter with utmost priority.
            <br>
        <br>
        However, our mission at Freedium remains unchanged, and your support is more crucial than ever. We are transitioning to Patreon, a platform that aligns with our values and offers us the freedom to share our work with you.
    <br>
<br>
Please join us on Patreon and continue to support our endeavors. Your contributions are invaluable to us, and we are committed to delivering the quality content you‚Äôve come to expect from Freedium.
<br>
<br>
Thank you for your understanding and unwavering support.
<br>
<br>
Support Us on Patreon
<br>
<br>
Warm regards, The Freedium Team
</p>
<a href="https://noref.io/#https://patreon.com/Freedium" target=_blank title=Patreon>
    <button class="bg-red-400 mx-1 text-white hover:bg-red-500 font-semibold py-1 px-2 rounded mt-2">Patreon</button>
</a>
<button class="bg-gray-300 mx-1 hover:bg-gray-400 text-gray-800 font-semibold py-1 px-2 rounded mt-2 close-button">Close</button>
<a href="https://codeberg.org/Freedium-cfd/web" target=_blank title=Codeberg>
    <button class="bg-blue-800 hover:bg-blue-900 mx-1 text-white font-semibold py-1 px-2 rounded mt-2">Source code - Codeberg</button>
</a>
<a href="https://github.com/Freedium-cfd/web" target=_blank title=GitHub></a>
<button class="bg-gray-700 hover:bg-gray-600 mx-1 text-white font-semibold py-1 px-2 rounded mt-2">Source code - GitHub</button>

</div>
</div>
<nav id=header class="fixed w-full z-9 top-0 dark:bg-gray-800 dark:text-white bg-white shadow" style="z-index: 999990;">
    
    <div id=progress class="h-1 z-20 top-0" style="background:linear-gradient(to right, #4dc0b5 var(--scroll), transparent 0)"></div>
    <div class="w-full md:max-w-4xl mx-auto flex flex-wrap items-center justify-between mt-0 py-3">
        <div class=pl-4>
            <a class="text-green-500 text-base no-underline hover:no-underline font-extrabold text-xl" href="/" onclick=navigateToOrigin()>Freedium</a>
        </div>
        <div class="block lg:hidden pr-4">
            <button id=nav-toggle class="flex items-center px-3 py-2 border rounded text-gray-500 dark:text-white border-gray-600 hover:text-gray-900 dark:hover:text-white hover:border-green-500 appearance-none focus:outline-none">
                <svg class="fill-current h-3 w-3" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
                    <title>Menu</title>
                    <path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z"></path>
                </svg>
            </button>
        </div>
        <div class="w-full flex-grow lg:flex lg:items-center lg:w-auto hidden lg:block mt-2 lg:mt-0 dark:bg-gray-800 bg-white" id=nav-content>
            <ul class="list-reset lg:flex justify-end flex-1 items-center">
                <li class=mr-3>
                    <a class="inline-block text-gray-600 dark:text-white no-underline hover:text-gray-900 dark:hover:text-white hover:text-underline py-2 px-4" href="https://codeberg.org/Freedium-cfd/web" target=_blank>Source code</a>
                </li>
                <li class=mr-3>
                    <a class="inline-block text-gray-600 dark:text-white no-underline hover:text-gray-900 dark:hover:text-white hover:text-underline py-2 px-4" href="https://freedium-miror-saqg.vercel.app/" target=_blank>Mirror 1</a>
                </li>
                <li class=mr-3>
                    <a class="inline-block text-gray-600 dark:text-white no-underline hover:text-gray-900 dark:hover:text-white hover:text-underline py-2 px-4" href="https://freedium-mirror.vercel.app/" target=_blank>Mirror 2</a>
                </li>
                <li class=mr-3>
                    <a class="inline-block text-gray-600 dark:text-white no-underline hover:text-gray-900 dark:hover:text-white hover:text-underline py-2 px-4" href="https://romans-status-page.vercel.app/status/freedium" target=_blank>Status page</a>
                </li>
                <li class=mr-3>
                    <a class="inline-block text-gray-600 dark:text-white no-underline hover:text-gray-900 dark:hover:text-white hover:text-underline py-2 px-4" href="https://noref.io/#https://patreon.com/Freedium" target=_blank>Patreon - Support us</a>
                </li>
            </ul>
        </div>
    </div>
</nav>
<div class="container w-full md:max-w-3xl mx-auto pt-20 break-words text-gray-900 dark:text-gray-200 bg-white dark:bg-gray-800">
    <div class="w-full px-4 md:px-6 text-xl text-gray-800 dark:text-gray-100 leading-normal" style=font-family:Georgia,serif;>
        <div class=font-sans>
            <p class="text-base md:text-sm text-green-500 font-bold pb-3">
                <a href="https://blog.stackademic.com/introduction-to-multimodal-deep-learning-c2d521d0a4cf#bypass" class="text-sm md:text-sm text-green-500 font-bold no-underline hover:underline ">&lt; Go to the original</a>
            </p>
            
                <img alt="Preview image" style="max-height: 65vh;
                            width: auto;
                            margin: auto" loading=eager role=presentation src="https://miro.medium.com/v2/resize:fit:700/0*QKJr2wJ-ZORnHdWi.png">
            
            <h1 class="font-bold font-sans break-normal text-gray-900 dark:text-gray-100 pt-6 pb-2 text-3xl md:text-4xl">Introduction to Multimodal Deep Learning</h1>
            <h2 class="font-medium font-sans break-normal text-gray-600 dark:text-gray-200 pt-1 text-1xl md:text-1xl">Basics of Multimodal Models</h2>
        </div>
        <div class="bg-gray-100 dark:bg-gray-600 border border-gray-300 m-2 mt-5">
            <div class="flex items-center space-x-4 p-4">
                <div class=flex-shrink-0>
                    <a href="https://medium.com/@abdulkaderhelwan" target=_blank title="Research Scientist, AI in Medicine, Technical Reviewer at ContentLab.io. https://bio.site/AbdulkaderH" class="block relative">
                        <img src="https://miro.medium.com/v2/resize:fill:88:88/0*tPXyW6sVZzf3le5D." alt="Abdulkader Helwan" class="rounded-full h-11 w-11 no-lightense">
                        <div class="absolute bottom-0 right-0 h-3 w-3 border-2 border-white bg-green-500 rounded-full"></div>
                    </a>
                </div>
                <div class=flex-grow>
                    <a href="https://medium.com/@abdulkaderhelwan" target=_blank title="Research Scientist, AI in Medicine, Technical Reviewer at ContentLab.io. https://bio.site/AbdulkaderH" class="block font-semibold text-gray-900 dark:text-white">Abdulkader Helwan</a>
                    <button class="text-sm text-white bg-green-500 px-3 py-1 rounded-lg mt-1 dark:bg-green-700">
                        <a href="https://medium.com/@abdulkaderhelwan" target=_blank title="Research Scientist, AI in Medicine, Technical Reviewer at ContentLab.io. https://bio.site/AbdulkaderH" class="block text-sm text-white">Follow</a>
                    </button>
                </div>
            </div>
            <div class="px-4 pb-2">
                <div class="flex flex-wrap items-center space-x-2 text-sm text-gray-500 dark:text-white">
                    
                        <a href="https://medium.com/stackademic" title="Stackademic is a learning hub for programmers, devs‚Ä¶" target=_blank class="flex items-center space-x-1">
                            <img src="https://miro.medium.com/v2/resize:fill:48:48/1*U-kjsW7IZUobnoy1gAp1UQ.png" alt=Stackademic class="h-4 w-4 rounded-full no-lightense">
                            <p>Stackademic</p>
                        </a>
                        <span>¬∑</span>
                    
                    <span class="text-gray-500 dark:text-white">~7 min read</span>
                    <span class=md:inline>¬∑</span>
                    <span class="text-gray-500 dark:text-white">April 15, 2024 (Updated: April 16, 2024)</span>
                    <span class=md:inline>¬∑</span>
                    <span class="text-yellow-500 dark:text-yellow-400">Free: No</span>
                </div>
            </div>
        </div>
        <div class="main-content mt-8">
            <p class="leading-8 mt-7">Humans rely on their five senses to perceive and make sense of the world. Each sense gathers information from a different source, providing a unique perspective on our surroundings. A modality, or way of experiencing something, plays a crucial role in this process. As artificial intelligence advances, it aims to replicate the complexity of the human brain.<p class="leading-8 mt-7">The human brain is capable of processing various modalities simultaneously through neural networks. When engaging in a conversation, these networks integrate inputs from audio, visual cues, text, and even smells. By fusing these modalities subconsciously, we can understand the speaker's message, emotions, and context. This holistic approach enhances our comprehension of the situation.<p class="leading-8 mt-7">To achieve human-like intelligence, AI must learn to interpret, reason, and fuse information from multiple modalities. Multimodal Deep Learning is a cutting-edge area of research that focuses on this challenge.<p class="leading-8 mt-7">In this article, we will discuss the multimodal deep learning models, fusion, and multimodal datasets.<h3 class="font-bold font-sans break-normal text-gray-900 dark:text-gray-100 text-1xl md:text-2xl pt-12">What is a Multimodal DL Model?</h3><p class="leading-8 mt-3">Multimodal machine learning involves training computer algorithms to enhance performance by utilizing various types of data. By combining different modalities such as images, video, audio, and text, AI models can better understand their surroundings. For instance, when recognizing emotions, it's not just about facial expressions but also about the tone and pitch of a person's voice.<p class="leading-8 mt-7">Unimodal models, which focus on a single modality, have shown great progress in fields like computer vision and natural language processing. However, their limitations have led to the development of multimodal models. The image below shows how unimodal models struggle with tasks like identifying sarcasm or hate speech, highlighting the importance of multimodal deep learning.<div class=mt-7><img alt=None style="margin: auto;" class="pt-5 lazy" role=presentation data-src="https://miro.medium.com/v2/resize:fit:700/1*MwMRLov4RtQODMAhjsxqGw.png"></div><figcaption class="mt-3 text-sm text-center text-gray-500 dark:text-gray-200">Unimodal models struggle to pick up on sarcasm because they only have access to half of the information in each modality. On the other hand, a multimodal model that analyzes both text and images can connect the dots and uncover the underlying message. <a class=text-base style="text-decoration: underline;" rel="" title="" href="https://ai.meta.com/blog/hateful-memes-challenge-and-data-set/" target=_blank>Source</a></figcaption><p class="leading-8 mt-7">The common modalities in multimodal deep learning are visual (images, videos), textual, and auditory (voice, sounds, music). Less common modalities consist of 3D visual data, depth sensor data, and LiDAR data (common in self-driving cars).<p class="leading-8 mt-7">In clinical practice, imaging modalities involve computed tomography (CT) scans and X-ray images, while non-image modalities consist of electroencephalogram (EEG) data. Sensor data such as thermal data or data from eye-tracking devices may also be considered.<p class="leading-8 mt-7">However, the most popular combinations are combinations of the three most popular modalities<ul class="list-disc pl-8 mt-2"><li class=mt-3>Image + Text<li class=mt-3>Image + Audio<li class=mt-3>Image + Text + Audio<li class=mt-3>Text + Audio</ul><h3 class="font-bold font-sans break-normal text-gray-900 dark:text-gray-100 text-1xl md:text-2xl pt-12">How do Multimodal Models Work?</h3><p class="leading-8 mt-3">Multimodal neural networks typically involve combining multiple unimodal neural networks. For instance, an audiovisual model may include two separate unimodal networks ‚Äî one for visual data and one for audio data. These unimodal networks process their inputs independently, a process known as encoding. Following encoding, the information from each model needs to be fused together using various techniques, from simple concatenation to attention mechanisms. The fusion of multimodal data is crucial for success. Once fusion occurs, a final "decision" network is trained on the combined information for the end task.<p class="leading-8 mt-7">In short, multimodal architectures consist of three main components:<p class="leading-8 mt-7">1. Unimodal encoders for each input modality.
2. A fusion network that merges features from each input modality.
3. A classifier that uses the fused data to make predictions.<p class="leading-8 mt-7">These components are referred to as the encoding module, fusion module, and classification module.<div class=mt-7><img alt=None style="margin: auto;" class="pt-5 lazy" role=presentation data-src="https://miro.medium.com/v2/resize:fit:700/1*dWIDgeWhVn5PY6hq2pocTQ.png"></div><figcaption class="mt-3 text-sm text-center text-gray-500 dark:text-gray-200">Example of a multimodal model. <em>Two unimodal neural networks models encode the different input modalities independently. fusion module combine the different modalities (optionally in pairs), and finally, the fused features are inserted into a classification network.</em> by author</figcaption><h4 class="font-bold font-sans break-normal text-gray-900 dark:text-gray-100 text-l md:text-xl pt-8">Encoders (Models 1 and 2)</h4><p class="leading-8 mt-3">When we encode data, our goal is to create meaningful representations. Typically, each type of data is processed by a separate encoder. However, it's common for inputs to be in the form of embeddings rather than raw data. For instance, text may use word2vec embeddings, while audio may use COVAREP embeddings. Multimodal embeddings like data2veq, which convert video, text, and audio data into embeddings in a high-dimensional space, have shown superior performance in various tasks.<p class="leading-8 mt-7">Deciding between joint representations and coordinated representations is a crucial choice. Joint representation methods are usually more effective when modalities are similar, and they are frequently preferred.<p class="leading-8 mt-7">When building multimodal networks, encoders are selected based on their performance in each domain, with more focus on designing the fusion method. Many studies opt for ResNets for visual data and RoBERTA for text.<h4 class="font-bold font-sans break-normal text-gray-900 dark:text-gray-100 text-l md:text-xl pt-8">Fusion</h4><p class="leading-8 mt-3">After feature extraction is done, the fusion module takes charge of combining each individual modality. The key to success lies in the method or architecture used for fusion.<p class="leading-8 mt-7">The most straightforward approach is to employ simple operations like concatenation or summation of the different unimodal representations. However, researchers and developers have delved into more advanced and fruitful techniques. One such method is the cross-attention layer mechanism, which has proven to be both recent and effective. It enables the capturing of cross-modal interactions and the fusion of modalities in a more meaningful manner. The equation provided below shows the workings of the cross-attention mechanism, assuming a basic understanding of self-attention:<div class=mt-7><img alt=None style="margin: auto;" class="pt-5 lazy" role=presentation data-src="https://miro.medium.com/v2/resize:fit:700/1*n4Co02uUgqqJZXJFz-6AXg.png"></div><p class="leading-8 mt-7">The attention score vector is represented by Œ±kl , with the softmax function denoted as s(.). The Key, Query, and Value matrices of the attention mechanism are represented by K, Q, and V respectively. To maintain symmetry, Œ±lk is also calculated, and the two can be combined to form an attention vector that represents the relationship between the two modalities (k,l) being considered.<p class="leading-8 mt-7">When dealing with three or more modalities, it's possible to utilize multiple cross-attention mechanisms to calculate every unique combination. For instance, with vision (V), text (T), and audio (A) modalities, we can create combinations like VT, VA, TA, and AVT to capture all potential cross-modal interactions. Additionally, after applying an attention mechanism, the cross-modal vectors can be concatenated to generate the fused vector F. Other operations like Sum(.), max(.), or pooling may also be employed for this purpose.<h4 class="font-bold font-sans break-normal text-gray-900 dark:text-gray-100 text-l md:text-xl pt-8">Classification Stage</h4><p class="leading-8 mt-3">After the fusion process is done, vector F is inputted into a classification model. Typically, this model is a neural network that consists of one or two hidden layers. The input vector F contains combined information from various modalities, resulting in a more comprehensive representation compared to the individual modalities V, A, and T. As a result, it enhances the predictive capability of the classifier.<h3 class="font-bold font-sans break-normal text-gray-900 dark:text-gray-100 text-1xl md:text-2xl pt-12">Multimodal Datasets</h3><p class="leading-8 mt-3">Data is essential for learning, and this holds true for multimodal machine learning as well. In order to advance this field, researchers and organizations have developed and shared numerous multimodal datasets. Below is a comprehensive list of some of the most popular datasets available:<p class="leading-8 mt-7"><strong>1. </strong><a class=text-base style="text-decoration: underline;" rel="" title="" href="https://github.com/tylin/coco-caption" target=_blank><strong>COCO-Captions Dataset:</strong></a><a class=text-base style="text-decoration: underline;" rel="" title="" href="https://github.com/tylin/coco-caption" target=_blank> </a>This dataset, created by Microsoft, consists of 330K images accompanied by short text descriptions. Its purpose is to enhance research in image captioning.<p class="leading-8 mt-7"><strong><a class=text-base style="text-decoration: underline;" rel="" title="" href="https://visualqa.org/" target=_blank>2. VQA:</a></strong> The Visual Question Answering multimodal dataset contains 265K images (visual modality) and at least three questions (text modality) for each image. Answering these questions requires a combination of visual understanding, language comprehension, and common sense knowledge. It is suitable for tasks such as visual-question answering and image captioning.<p class="leading-8 mt-7"><strong>3. </strong><a class=text-base style="text-decoration: underline;" rel="" title="" href="http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/" target=_blank><strong>CMU-MOSEI: </strong></a>The Multimodal Opinion Sentiment and Emotion Intensity (MOSEI) dataset is designed for human emotion recognition and sentiment analysis. It includes 23,500 sentences spoken by 1,000 YouTube speakers. This dataset combines video, audio, and text modalities, making it ideal for training models on the three most popular data modalities.<p class="leading-8 mt-7"><strong>4. </strong><a class=text-base style="text-decoration: underline;" rel="" title="" href="https://www.thesocialiq.com/" target=_blank><strong>Social-IQ:</strong></a> This dataset is perfect for training deep learning models in visual reasoning, multimodal question answering, and social interaction understanding. It consists of 1250 audio videos that have been meticulously annotated (on the action level) with questions and answers (text modality) related to the actions depicted in each scene.<p class="leading-8 mt-7"><strong>5. </strong><a class=text-base style="text-decoration: underline;" rel="" title="" href="https://deepmind.google/" target=_blank><strong>Kinetics 400/600/700:</strong></a> This audiovisual dataset comprises a collection of YouTube videos for human action recognition. It includes video (visual modality) and sound (audio modality) of people performing various actions, such as playing music, hugging, and playing sports. The dataset is suitable for tasks like action recognition, human pose estimation, and scene understanding.<p class="leading-8 mt-7">These datasets provide valuable resources for researchers and practitioners in the field of multimodal deep learning. By utilizing these datasets, advancements can be made in various areas of multimodal machine learning.<h3 class="font-bold font-sans break-normal text-gray-900 dark:text-gray-100 text-1xl md:text-2xl pt-12">Last Words</h3><blockquote class="px-5 pt-3 pb-3 mt-5 shadow-lf"><p style="font-style: italic;"><strong><em>If you like the article and would like to support me make sure to:
üì∞ View more content on my </em></strong><em><a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://medium.com/@abdulkaderhelwan" target=_blank><strong>medium</strong></a></em><strong><em> profile and </em></strong><em><a class=text-base style="text-decoration: underline;" rel="noopener ugc nofollow noopener" title="" href="https://emojipedia.org/clapping-hands" target=_blank><strong>üëè</strong></a></em><strong><em>Clap for this article</em></strong><em><a class=text-base style="text-decoration: underline;" rel="noopener ugc nofollow noopener" title="" href="https://www.blogger.com/blog/post/edit/6962154748903383574/6194706827742709853#" target=_blank><strong>
</strong></a></em><strong><em>üöÄüëâ Read more </em></strong><em><a class=text-base style="text-decoration: underline;" rel="" title="" href="https://medium.com/p/9ac268fa1186" target=_blank><strong>related articles</strong></a></em><strong><em> to this one on </em></strong><strong><a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://medium.com/@abdulkaderhelwan" target=_blank><em>Medium</em></a></strong></blockquote><blockquote class="px-5 pt-3 pb-3 mt-5 shadow-lf"><p style="font-style: italic;"><em><strong>Please consider subscribing:</strong></em></blockquote>
<div class="border border-gray-300 p-2 mt-7 items-center overflow-hidden"><a rel="noopener follow" href="https://abdulkaderhelwan.medium.com/subscribe" target=_blank> <div class="flex flex-row justify-between p-2 overflow-hidden"><div class="flex flex-col justify-center p-2"><h2 class="text-black dark:text-gray-100 text-base font-bold">Get an email whenever Abdulkader Helwan publishes.</h2><div class="mt-2 block"><h3 class="text-grey-darker text-sm">Get an email whenever Abdulkader Helwan publishes. By signing up, you will create a Medium account if you don't already‚Ä¶</h3></div><div class=mt-5 style=""><p class="text-grey-darker text-xs">medium.com</div></div><div class="relative flex flew-row h-40 w-60"><div class="lazy absolute inset-0 bg-cover bg-center" data-bg="https://miro.medium.com/v2/resize:fit:320/0*rAtxTaJJ-Tm8oakB"></div></div></div> </a></div><h3 class="font-bold font-sans break-normal text-gray-900 dark:text-gray-100 text-1xl md:text-2xl pt-12">Stackademic üéì</h3><p class="leading-8 mt-3">Thank you for reading until the end. Before you go:<ul class="list-disc pl-8 mt-2"><li class=mt-3>Please consider <strong>clapping</strong> and <strong>following</strong> the writer! üëè<li class=mt-3>Follow us <a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://twitter.com/stackademichq" target=_blank><strong>X</strong></a><strong> | </strong><a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://www.linkedin.com/company/stackademic" target=_blank><strong>LinkedIn</strong></a><strong> | </strong><a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://www.youtube.com/c/stackademic" target=_blank><strong>YouTube</strong></a><strong> | </strong><a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://discord.gg/in-plain-english-709094664682340443" target=_blank><strong>Discord</strong></a><li class=mt-3>Visit our other platforms: <a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://plainenglish.io" target=_blank><strong>In Plain English</strong></a><strong> | </strong><a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://cofeed.app/" target=_blank><strong>CoFeed</strong></a><strong> | </strong><a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://venturemagazine.net/" target=_blank><strong>Venture</strong></a><strong> | </strong><a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://blog.cubed.run" target=_blank><strong>Cubed</strong></a><li class=mt-3>More content at <strong><a class=text-base style="text-decoration: underline;" rel=noopener title="" href="https://stackademic.com" target=_blank>Stackademic.com</a></strong></ul>
        </div>
        <div class="flex flex-wrap gap-2 mt-5">
            <a title=Multimodal target=_blank href="https://medium.com/tag/multimodal"><span class="text-green-500 bg-green-100 px-2 py-1 rounded-full text-xs dark:bg-green-800 dark:text-gray-100">#multimodal</span></a><a title=AI target=_blank href="https://medium.com/tag/ai"><span class="text-green-500 bg-green-100 px-2 py-1 rounded-full text-xs dark:bg-green-800 dark:text-gray-100">#ai</span></a><a title="Multimodal Learning" target=_blank href="https://medium.com/tag/multimodal-learning"><span class="text-green-500 bg-green-100 px-2 py-1 rounded-full text-xs dark:bg-green-800 dark:text-gray-100">#multimodal-learning</span></a><a title=Encoder target=_blank href="https://medium.com/tag/encoder"><span class="text-green-500 bg-green-100 px-2 py-1 rounded-full text-xs dark:bg-green-800 dark:text-gray-100">#encoder</span></a><a title=Fusion target=_blank href="https://medium.com/tag/fusion"><span class="text-green-500 bg-green-100 px-2 py-1 rounded-full text-xs dark:bg-green-800 dark:text-gray-100">#fusion</span></a>
        </div>
        <div class="container w-full md:max-w-3xl mx-auto pt-12"></div>
    </div>
    <style>
.main-content {
 letter-spacing: -0.06px;
 font-family: source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif;
}
code {
    background-color: #e3e2e2;
}
pre {
    font-size: 75%;
    background-color: #e3e2e2;
}
p code, ul code, li code {
    font-size: 75%;
}
    </style>
    <script>
document.addEventListener('DOMContentLoaded', (event) => {
  hljs.highlightAll();

  document.querySelectorAll('pre code').forEach((el) => {
         code = el.textContent;  
          el = el.parentElement;
         el.innerHTML = '<button class="hljs-copy p-1 bg-gray-300 dark:bg-black">Copy</button>' + el.innerHTML; // append copy button
        el.getElementsByClassName('hljs-copy')[0].contentCopy = code;
         el.getElementsByClassName('hljs-copy')[0].addEventListener("click", function () {
             this.innerText = 'Copying..';
             if (!navigator.userAgent.toLowerCase().includes('safari')) {
                 navigator.clipboard.writeText(this.contentCopy);
             } else {
                 prompt("Clipboard (Select: ‚åò+a > Copy:‚åò+c)", this.contentCopy);
             }
             this.innerText = 'Copied!';
             button = this;
             setTimeout(function () {
                 button.innerText = 'Copy';
             }, 1500)
         });
});
  });
    </script>
    <style>
     .hljs-copy {
         float: right;
         cursor: pointer;
     }
    </style>
<div id=problemModal class="modal hidden fixed inset-0 w-full h-full flex items-center justify-center overflow-y-auto bg-black bg-opacity-50" style="z-index: 999999">
    <div class="modal-container w-11/12 md:max-w-xl mx-auto rounded shadow-lg max-h-screen">
        <div class="modal-content bg-white dark:bg-gray-800 dark:text-white my-8 py-4 text-left px-6">
            <h1 class="text-3xl font-bold">Reporting a Problem</h1>
            <div class=mt-3>
                <p>
                    Sometimes we have problems displaying some Medium posts.
                <br>
            <br>
        </p>
        <p>If you have a problem that some images aren't loading - try using VPN. Probably you have problem with access to Medium CDN (or fucking Cloudflare's bot detection algorithms are blocking you).</p>
    </div>
    <form action=# method=POST class=mt-4 id=problem-form>
        <div class=mb-4>
            <label for=problem-description class="block text-gray-700 dark:text-white font-bold mb-2">Problem Description</label>
            <textarea id=problem-description name=problem-description placeholder="Describe your problem here..." class="shadow appearance-none border rounded w-full py-2 px-3 text-gray-700 leading-tight focus:outline-none focus:shadow-outline" rows=4 required=""></textarea>
        </div>
        <div>
            <button type=submit class="m-2 bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded focus:outline-none focus:shadow-outline">Submit</button>
            <button type=button class="m-2 modal-close bg-gray-500 hover:bg-gray-700 text-white font-bold py-2 px-4 rounded focus:outline-none focus:shadow-outline">Cancel</button>
        </div>
    </form>
</div>
</div>
</div>
<script>
    tailwind.config = {
        darkMode: 'class',
    }

  function navigateToOrigin() {
    window.location.href = window.location.origin;
  }
if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
  // document.documentElement.classList.add('dark');
  document.getElementById('darkIcon').classList.remove('hidden');
  document.getElementById('lightIcon').classList.add('hidden')
} else {
  // document.documentElement.classList.remove('dark')
  document.getElementById('lightIcon').classList.remove('hidden');
  document.getElementById('darkIcon').classList.add('hidden');
}

  document.getElementById('darkModeToggle').addEventListener('click', function() {

    if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
  document.documentElement.classList.remove('dark');
  document.getElementById('darkIcon').classList.add('hidden');
  document.getElementById('lightIcon').classList.remove('hidden')
  document.documentElement.style.cssText = "--lightense-backdrop: white;";
  localStorage.setItem("theme", "light")
} else {
  document.documentElement.classList.add('dark')
  document.getElementById('lightIcon').classList.add('hidden');
  document.getElementById('darkIcon').classList.remove('hidden');
  document.documentElement.style.cssText = "--lightense-backdrop: black;";
  localStorage.setItem("theme", "dark")
}
});
</script>
<script>
                const openModalButton = document.getElementById('openProblemModal');
                const closeModalButton = document.querySelector('.modal-close');
                const modal = document.getElementById('problemModal');
                const problemDescriptionInput = document.getElementById('problem-description');
                const submitButton = document.querySelector('form button');
                const body = document.querySelector('body');

                openModalButton.addEventListener('click', () => {
                    body.classList.add('overflow-hidden'); // Prevent scrolling on the body
            modal.classList.remove('hidden');
        });

        closeModalButton.addEventListener('click', () => {
            body.classList.remove('overflow-hidden'); // Re-enable scrolling on the body
            modal.classList.add('hidden');
        });

        modal.addEventListener('click', (e) => {
            if (e.target === modal) {
                modal.classList.add('hidden');
                body.classList.remove('overflow-hidden');
            }
        });

        function navigateNoCache() {
                window.location.href = `/render-no-cache${window.location.pathname}`;
        }

        const submitForm = async (event) => {
            event.preventDefault();

            console.log('Form submiting is started!');
            submitButton.disabled = true;

            // Get the problem description from the input field
            const problemDescription = problemDescriptionInput.value;
            const currentPage = window.location.href;

            try {
                // Send a POST request to the "report-problem" API endpoint
                const response = await fetch('/report-problem', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({ description: problemDescription, page: currentPage }),
                });

                if (response.ok) {
                    // Report submitted successfully, you can add a success message or further actions here
                    console.log('Problem report submitted successfully.');
                    modal.classList.add('hidden'); // Close the modal
                } else {
                    // Handle errors, such as non-200 responses
                    console.error('Failed to submit problem report.');
                    submitButton.disabled = false;
                }
            } catch (error) {
                // Handle network errors or other exceptions
                console.error('An error occurred:', error);
                submitButton.disabled = false;
            }
        };

        document.getElementById('problem-form').onsubmit = submitForm;
</script>
<script>
const h = document.documentElement, b = document.body;
const st = 'scrollTop';
const sh = 'scrollHeight';
const progress = document.getElementById('progress');
const header = document.getElementById('header');
const navcontent = document.getElementById('nav-content');

document.addEventListener('scroll', function () {
  /* Refresh scroll % width */
  const scroll = (h[st] || b[st]) / ((h[sh] || b[sh]) - h.clientHeight) * 100;
  progress.style.setProperty('--scroll', scroll + '%');

  /* Apply classes for slide in bar */
  const shouldAddClass = window.scrollY > 10;
});

		document.getElementById('nav-toggle').onclick = function() {
			document.getElementById("nav-content").classList.toggle("hidden");
		}

  window.addEventListener('load', function () {
       Lightense('img:not(.no-lightense)');
  }, false);
</script>


<script>
            var lazyLoadInstance = new LazyLoad({
                    callback_loaded: function(element) {
        Lightense(element);
    },
    callback_error: (img) => {
        console.log(img);
        if (img.hasAttribute("data-src")) {
            if (img.attributes["data-src"].value.startsWith("https://miro.medium.com/v2/")) {
            img.setAttribute("src", img.attributes["data-src"].value.replace("https://miro.medium.com/v2/", "https://freedium.cfd/@miro/v2/" ));
            }
        }
  }
});
</script>
<script>
    function navigateToOrigin() {
      window.location.href = window.location.origin;
    }
</script>
<script>
      document.addEventListener('DOMContentLoaded', () => {
        const notificationContainer = document.querySelector('.notification-container');
        const closeButton = document.querySelector('.close-button');
        const notificationFlagString = "showNotification-buymeacoffe-ban-block"
        const body = document.querySelector('body');

        function showNotification() {
          if (!localStorage.getItem(notificationFlagString)) {
                  notificationContainer.style.display = 'block';
                  body.classList.add('overflow-hidden');
          }
        }

        function hideNotification() {
          localStorage.setItem(notificationFlagString, 'false');
          notificationContainer.style.display = 'none';
          body.classList.remove('overflow-hidden');
        }

        // Close button functionality
        closeButton.addEventListener('click', () => {
          hideNotification();
        });

        showNotification();
      });
</script></div>