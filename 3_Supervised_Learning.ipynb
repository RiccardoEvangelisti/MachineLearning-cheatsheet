{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "&rarr; find correlations between variables; predicting/estimating output variables based on one or more input variables\n",
    "\n",
    "- Given an input ($x$, samples of one or more variables) and an output response ($y$, samples of one or more other variables of the dataset), we have to find the relationship ($f$) between them: $y = f(x) + e$\n",
    "    - where \"$e$\" is called **bias**, a random error that is independent of input and has mean zero\n",
    "- The predicted/estimated function ($\\hat{f}$) generates, with the same input ($x$), a resulting output ($\\hat{y}$, the estimated samples): $\\hat{y}=\\hat{f}(x)$\n",
    "- The accuracy lies on the similarity between $y$ and $\\hat{y}$ (between the real samples and the predicted ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&rarr; **Regression**: predicting continuous (quantitative) value <br>\n",
    "&rarr; **Classification**: predicting a discrete value, that corresponds to whether your sample belongs to a class or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Over/Underfitting\n",
    "\n",
    "- The estimation of $f$ can be made by a parametric approach: it is generally much easier to estimate a set of parameters, than it is to fit an entirely arbitrary function.\n",
    "    -  ***Overfitting***: the model we choose will usually not match the true unknown form of $f$. If the model is too flexible (too similar to $f$), the training error will be too low and the test error can be high. The solution is to change/remove parameters or simplify the model.\n",
    "    - ***Underfitting*** happens when you underestimate the model.\n",
    "- It is important to decide for any given set of data which method produces the best results\n",
    "    - **Linear** models are simple but often too inaccurate\n",
    "    - **Highly non-linear** models can potentially provide more accurate predictions, but far more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bias–Variance tradeoff\n",
    "- The ***bias*** error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "- The ***variance*** is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Classification Metrics [(doc)](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "(for binary predictions)\n",
    "|  |  | |\n",
    "| --- | --- | --- |\n",
    "| **Accuracy** | Mean between the error rate of all the predictions; error rate is 1 if wrong prediction, 0 otherwise <br> Can be heavily misleading when classes are highly imbalanced | $$ (TP + TN) / N $$ |\n",
    "| **Precision** | The ratio between correctly predicted positive observations and the total predicted positive observations. <br>High precision relates to the low false positive rate and it helps when the costs of false positives are high | $$  TP/(TP + FP) $$ |\n",
    "| **Recall/Sensitivity** | The ratio between correctly predicted positive observations and all observations in the actual class. <br>High recall means a low false-negative rate and it helps when the cost of false negatives is high | $$ TP/(TP+FN) $$ |\n",
    "| **Specificity** | The ratio between correctly predicted negative observations and all observations in the actual class | $$TN/(TN+FP)$$ |\n",
    "| **F1 score** |  The weighted average of Precision and Recall. <br> Is always interesting, because has higher values when precision and recall are reasonably balanced. <br>A good score means that there are low false positives and low false negatives, so the model is correctly identifying real threats and it is not disturbed by false alarms. <br> If the costs of errors on positives and negatives are significantly different, then it is necessary to evaluate precision and recall | $$2\\frac{Precision*Recall}{Precision+Recall}$$ |\n",
    "\n",
    "- For Multi-Class predictions:\n",
    "    - Precision, Recall and F1–score are intrinsically defined for a single class\n",
    "    - when a single value is necessary to optimize the hyperparameters, as in GridSearch, if we need to maximize one of Precision, Recall, F1–score, an *average value* is required:\n",
    "        - *macro average*: the measure of each class has the same impact on the average value, therefore in case of imbalance the minority classes have an influence bigger than their frequency (of appearance)\n",
    "        - *weighted average*: the measure of each class influences the result in proportion to its frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics [(doc)](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "\n",
    "|  |  |\n",
    "| --- | --- |\n",
    "| **Residual Sum of Squares (RSS)** | the sum of the squares of all residuals, which are the distance between the predicted value and the real one. A problem is the value that increases with the number of samples. $$ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$ |\n",
    "| **Mean Squared Error (MSE)** | the mean of all the distances (squared) between a predicted sample and the real one $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$ |\n",
    "| **Mean Absolute Error (MAE)** | $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} \\|y_i - \\hat{y_i}\\| $$ |\n",
    "| **Residual Standard Error (RSE)** | absolute measure of lack of fit of the model. RSS normalized with respect to n and p: $$ RSE = \\sqrt{\\frac{1}{n - p - 2} \\times RSS} $$ |\n",
    "| **Explained Sum of Squares (ESS)** | quantity of variance explained by the model. How much the predicted data differ from the mean: $$ ESS = \\sum_{i=1}^n (\\hat{y_i} - \\overline{y})^2 $$ |\n",
    "| **Total Sum of Squares (TSS)** | Total variance in the response data. How much the real data differ from the mean: $$ TSS = ESS + RSS = \\sum_{i=1}^n (y_i - \\overline{y})^2 $$ |\n",
    "| **R^2** |     $$ R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS} $$ - close to 1: the model fits well the data <br> - close to 0: linear model is wrong, or bias error is high<br> - It is not meaningful for non–linear or non–algebraic regression models|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "df_X = wine[\"data\"]\n",
    "df_y = wine[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=random_state)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test split\n",
    "\n",
    "- The Training Set is the set of samples used for training our model\n",
    "- The Validation Set is the set of samples we used to tune the hyperparameters of our model. ***Calibrating the hyperparameters on the test set means overestimating the performance***\n",
    "- The Test Set is the set of samples on which to evaluate the final performance of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t (178, 13)\n",
      "X_train: (89, 13) , % 50.0\n",
      "X_val:\t (30, 13) , % 16.853932584269664\n",
      "X_test:\t (59, 13) , % 33.146067415730336\n",
      "y:\t (178,)\n",
      "y_train: (89,)\n",
      "y_val:\t (30,)\n",
      "y_test:\t (59,)\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#########################################################################\n",
    "# CLASSIC SPLIT\n",
    "#########################################################################\n",
    "\n",
    "# Split X and y into train and test, with test size of 33% of the total\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.33, random_state=random_state)\n",
    "# Split X_train and y_train into train and valuation, with valuation size of 25% of the train total\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=random_state)\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "# PERSONALIZED train_validation_test_split()\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "def train_validation_test_split(*arrays, train_size=None, validation_size=None, test_size=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Splits a dataset into train, validation, and test sets based on the provided proportions.\n",
    "\n",
    "    # Parameters:\n",
    "        *arrays : sequence of indexables with same length / shape[0]\n",
    "            Allowed inputs are lists, numpy arrays, scipy-sparse\n",
    "            matrices or pandas dataframes.\n",
    "        train_size (float):\n",
    "            Proportion of the dataset for the training set (0.0 to 1.0).\n",
    "        validation_size (float):\n",
    "            Proportion of the dataset for the validation set (0.0 to 1.0).\n",
    "        test_size (float):\n",
    "            Proportion of the dataset for the test set (0.0 to 1.0).\n",
    "\n",
    "    # Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test: Split datasets.\n",
    "    \"\"\"\n",
    "    if train_size and validation_size:\n",
    "        test_size = 1.0 - train_size - validation_size\n",
    "    elif test_size and validation_size:\n",
    "        train_size = 1.0 - test_size - validation_size\n",
    "    else:\n",
    "        raise ValueError(\"Provide either train and validation sizes or test and validation sizes.\")\n",
    "\n",
    "    if test_size < 0 or validation_size < 0 or train_size < 0:\n",
    "        raise ValueError(\"The total sum must be a value < 1.0\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(*arrays, test_size=test_size, random_state=random_state)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        test_size=validation_size * (X_train.shape[0] + X_test.shape[0]) / X_train.shape[0],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n",
    "    df_X, df_y, test_size=0.33, validation_size=0.1675\n",
    ")\n",
    "\n",
    "print(\"X:\\t\", df_X.shape)\n",
    "print(\"X_train:\", X_train.shape, \", %\", X_train.shape[0] * 100 / df_X.shape[0])\n",
    "print(\"X_val:\\t\", X_val.shape, \", %\", X_val.shape[0] * 100 / df_X.shape[0])\n",
    "print(\"X_test:\\t\", X_test.shape, \", %\", X_test.shape[0] * 100 / df_X.shape[0])\n",
    "print(\"y:\\t\", df_y.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_val:\\t\", y_val.shape)\n",
    "print(\"y_test:\\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Cross Validation with *Train-Validation-Test Split*\n",
    "- Train and Validation sets do not change during the search for optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on VALIDATION SET: 0.96667 K = 1\n",
      "Accuracy on VALIDATION SET: 0.70000 K = 6\n",
      "Accuracy on VALIDATION SET: 0.76667 K = 11\n",
      "Accuracy on VALIDATION SET: 0.76667 K = 16\n",
      "Accuracy on VALIDATION SET: 0.76667 K = 21\n",
      "\n",
      "Best validation model: K = 1\n",
      "Accuracy on TEST SET: 0.75000\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "dataset = datasets.load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset[\"data\"], dataset[\"target\"], test_size=0.4, random_state=random_state\n",
    ")\n",
    "\n",
    "# Parameters to keep the best results\n",
    "clf = dict()\n",
    "best_acc = 0\n",
    "best_acc_ind = -1\n",
    "\n",
    "\n",
    "for i in range(1, 22, 5):\n",
    "    clf[i] = KNeighborsClassifier(n_neighbors=i)\n",
    "\n",
    "    # Fit on TRAIN SET\n",
    "    clf[i].fit(X_train, y_train)\n",
    "\n",
    "    # Score on VALIDATION SET\n",
    "    acc = clf[i].score(X_val, y_val)\n",
    "\n",
    "    if best_acc < acc:\n",
    "        best_acc = acc\n",
    "        best_acc_ind = i\n",
    "\n",
    "    print(\"Accuracy on VALIDATION SET: {:.5f}\".format(acc), \"K =\", i)\n",
    "\n",
    "print(\"\\nBest validation model: K =\", best_acc_ind)\n",
    "\n",
    "# Test on TEST SET\n",
    "acc = clf[best_acc_ind].score(X_test, y_test)\n",
    "print(\"Accuracy on TEST SET: {:.5f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cross Validation with *k-Fold* and *cross_val_score()*\n",
    "- Train and Validation sets do change during the search for optimal hyperparameters\n",
    "- **Leave-One-Out** Cross-Validation. Leave out one sample, fit the model on the remaining samples, and finally validate(/test) the model on the left-out sample. Then repeat for every sample, and calculate the final error as the mean of all errors. It's very very expensive because there are as many iterations as number of samples.\n",
    "- ***k-Fold*** Cross Validation. Randomly divide the set of samples into \"k\" groups (folds) of approximately equal size. The first fold is left out, the remaining are fitted, and finally the model is tested on the fold left out. Then repeat for every fold, and calculate the final error as the mean of all errors. Typically 3 to 5 folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Preprocessing and *Data Leakage*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.962 kernel = linear\n",
      "Cross validation score: 0.934 kernel = poly\n",
      "Cross validation score: 0.962 kernel = rbf\n",
      "Best Kernel: linear\n",
      "Test set accuracy: 0.98611\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random_state = 42\n",
    "dataset = datasets.load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset[\"data\"], dataset[\"target\"], test_size=0.4, random_state=random_state\n",
    ")\n",
    "\n",
    "# PREPROCESSING\n",
    "# WARNING!!!: preprocessing the data with Min-Max or StandardScaling (in general with all normalizations that follow the vertical axes) and then performing a cross_val_score() introduces *DATA LEAKAGE*. This is because cross_val_score() splits the train set in \"cv\" folds at each iteration, and uses only \"cv-1\" for training. The left-out is used for validation, that is to calculate the error. But this left-out fold is already preprocessed with the scaler taken from all the \"cv\" folds, and not only on the \"cv-1\" train folds. Therefore this left-out validation fold is preprocessed even with his own data, *and this means overstimate the performance*.\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_p = scaler.transform(X_train)\n",
    "X_test_p = scaler.transform(X_test)\n",
    "\n",
    "# Parameters to keep the best results\n",
    "best_acc = 0\n",
    "bestK = \"linear\"\n",
    "\n",
    "# KFold\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Iterate on an hyperparameter\n",
    "for kernel in [\"linear\", \"poly\", \"rbf\"]:\n",
    "    clf = SVC(kernel=kernel, random_state=random_state)\n",
    "    # Cross Validate\n",
    "    scores = cross_val_score(clf, X_train_p, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "    # save best result so far\n",
    "    acc = np.mean(scores)\n",
    "    print(\n",
    "        \"Cross validation score: {:.3f}\".format(acc),\n",
    "        \"kernel =\",\n",
    "        kernel,\n",
    "    )\n",
    "    if best_acc < acc:\n",
    "        best_acc = acc\n",
    "        bestK = kernel\n",
    "\n",
    "\n",
    "print(\"Best Kernel:\", bestK)\n",
    "clf = SVC(kernel=bestK, random_state=random_state)\n",
    "clf.fit(X_train_p, y_train)\n",
    "print(\"Test set accuracy: {:.5f}\".format(clf.score(X_test_p, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cross Validation with GridSearch\n",
    "- Find the best hyperparameters, trying every combination of them\n",
    "- Useful doc for advanced operations on Pipeline: [doc](https://amueller.github.io/aml/01-ml-workflow/12-pipelines-gridsearch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Preprocessing and *no Data Leakage*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParamGrid length: 108\n",
      "\n",
      "Params: {'scaler': MinMaxScaler(), 'svc__C': 100, 'svc__kernel': 'linear'}\n",
      "Score \"accuracy\": 0.9766081871345029\n",
      "\n",
      "Params: {'scaler': StandardScaler(), 'svc__C': 10, 'svc__kernel': 'linear'}\n",
      "Score \"make_scorer(precision_score, zero_division=0)\": 0.9811320754716981\n",
      "\n",
      "Params: {'scaler': MinMaxScaler(), 'svc__C': 100, 'svc__kernel': 'linear'}\n",
      "Score \"recall_macro\": 0.9748677248677249\n",
      "\n",
      "Params: {'scaler': MinMaxScaler(), 'svc__C': 100, 'svc__kernel': 'linear'}\n",
      "Score \"f1_macro\": 0.9748677248677249\n",
      "\n",
      "Params: {'scaler': MinMaxScaler(), 'svc__C': 100, 'svc__kernel': 'linear'}\n",
      "Score \"make_scorer(mean_squared_error, greater_is_better=False, squared=False)\": -0.1529438225803745\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, preprocessing\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, precision_score\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split, GridSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "random_state = 42\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset[\"data\"], dataset[\"target\"], test_size=0.3, random_state=random_state\n",
    ")\n",
    "\n",
    "# Grid of hyperparameters\n",
    "param_grid = {\n",
    "    \"scaler\": [\n",
    "        \"passthrough\",\n",
    "        preprocessing.StandardScaler(),\n",
    "        preprocessing.MinMaxScaler(),\n",
    "        preprocessing.Normalizer(norm=\"l1\"),\n",
    "        preprocessing.Normalizer(norm=\"l2\"),\n",
    "        preprocessing.Normalizer(norm=\"max\"),\n",
    "    ],\n",
    "    \"svc__kernel\": [\"linear\", \"poly\", \"rbf\"],\n",
    "    \"svc__C\": [0.01, 0.1, 1, 10, 100, 500],\n",
    "}\n",
    "\n",
    "# ParamGrid Length\n",
    "print(\"ParamGrid length: {}\\n\".format(len(list(ParameterGrid(param_grid)))))\n",
    "\n",
    "# Iterate over different scores\n",
    "scorings = [\n",
    "    \"accuracy\",\n",
    "    make_scorer(precision_score, zero_division=0),\n",
    "    \"recall_macro\",\n",
    "    \"f1_macro\",\n",
    "    make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n",
    "]\n",
    "for score in scorings:\n",
    "    pipe = Pipeline([(\"scaler\", None), (\"svc\", svm.SVC(random_state=random_state))])\n",
    "    gs = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid=param_grid,\n",
    "        scoring=score,\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "    )\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    print(\n",
    "        'Params: {}\\nScore \"{}\": {}\\n'.format(\n",
    "            gs.best_params_,\n",
    "            score,\n",
    "            gs.score(X_test, y_test),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Iterate over multiple estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'est': RandomForestClassifier(n_jobs=-1), 'est__criterion': 'log_loss', 'est__max_depth': 40}\n",
      "Score: 0.962044983722995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset[\"data\"], dataset[\"target\"], test_size=0.3, random_state=random_state\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"est\": [RandomForestClassifier(n_jobs=-1)],\n",
    "        \"est__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"est__max_depth\": [10, 30, 40, 50],\n",
    "    },\n",
    "    {\n",
    "        \"est\": [KNeighborsClassifier(n_jobs=-1)],\n",
    "        \"est__weights\": [\"uniform\", \"distance\"],\n",
    "        \"est__n_neighbors\": [*range(1, X_test.shape[0] // 2)],\n",
    "    },\n",
    "]\n",
    "\n",
    "pipe = Pipeline([(\"est\", None)])\n",
    "\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=\"f1_macro\", cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Params: {}\\nScore: {}\\n\".format(\n",
    "        grid.best_params_,\n",
    "        grid.score(X_test, y_test),\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
